{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div class=\"clearfix\" style=\"padding: 10px; padding-left: 0px\">\n<a href=\"http://bombora.com\"><img src=\"https://app.box.com/shared/static/e0j9v1xjmubit0inthhgv3llwnoansjp.png\" width=\"200px\" class=\"pull-right\" style=\"display: inline-block; margin: 5px; vertical-align: middle;\"></a>\n<h1> Bombora Data Science: <br> *Interview Exam* </h1>\n</div>\n\n<img width=\"200px\" src=\"https://app.box.com/shared/static/15slg1mvjd1zldbg3xkj9picjkmhzpa5.png\">"},{"metadata":{},"cell_type":"markdown","source":"---\n# Welcome\n\nWelcome! This notebook contains interview exam questions referenced in the *Instructions* section in the `README.md`—please read that first, *before* attempting to answer questions here.\n\n<div class=\"alert alert-info\" role=\"alert\" style=\"margin: 10px\">\n<p style=\"font-weight:bold\">ADVICE</p>\n<p>*Do not* read these questions, and panic, *before* reading the instructions in `README.md`.</p>\n</div>\n\n<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n<p style=\"font-weight:bold\">WARNING</p>\n\n<p>If using <a href=\"https://try.jupyter.org\">try.jupyter.org</a> do not rely on the server for anything you want to last - your server will be <span style=\"font-weight:bold\">deleted after 10 minutes of inactivity</span>. Save often and rember download notebook when you step away (you can always re-upload and start again)!</p>\n</div>\n\n\n## Have fun!\n\nRegardless of outcome, getting to know you is important. Give it your best shot and we'll look forward to following up!"},{"metadata":{},"cell_type":"markdown","source":"# Exam Questions"},{"metadata":{},"cell_type":"markdown","source":"## 1. Algo + Data Structures"},{"metadata":{},"cell_type":"markdown","source":"### Q 1.1: Fibionacci\n![fib image](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Fibonacci_spiral_34.svg/200px-Fibonacci_spiral_34.svg.png)"},{"metadata":{},"cell_type":"markdown","source":"#### Q 1.1.1\nGiven $n$ where $n \\in \\mathbb{N}$ (i.e., $n$ is an integer and $n > 0$), write a function `fibonacci(n)` that computes the Fibonacci number $F_n$, where $F_n$ is defined by the recurrence relation:\n\n$$ F_n = F_{n-1} + F_{n-2}$$\n\nwith initial conditions of:\n\n$$ F_1 = 1,  F_2 = 1$$"},{"metadata":{},"cell_type":"markdown","source":"#### Q 1.1.2\nWhat's the complexity of your implementation?"},{"metadata":{},"cell_type":"markdown","source":"#### Q 1.1.3\nConsider an alternative implementation to compute Fibonacci number $F_n$ and write a new function, `fibonacci2(n)`."},{"metadata":{},"cell_type":"markdown","source":"#### Q 1.1.4\nWhat's the complexity of your implementation?"},{"metadata":{},"cell_type":"markdown","source":"#### Q 1.1.5\nWhat are some examples of optimizations that could improve computational performance?\n"},{"metadata":{},"cell_type":"markdown","source":"### Q 1.2: Linked List\n![ll img](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Singly-linked-list.svg/500px-Singly-linked-list.svg.png)"},{"metadata":{},"cell_type":"markdown","source":"#### Q 1.2.1\nConsider a [singly linked list](https://en.wikipedia.org/wiki/Linked_list), $L$. Write a function `is_palindrome(L)` that detects if $L$ is a [palindrome](https://en.wikipedia.org/wiki/Palindrome), by returning a bool, `True` or `False`.\n"},{"metadata":{},"cell_type":"markdown","source":"#### Q 1.2.2\nWhat is the complexity of your implementation?"},{"metadata":{},"cell_type":"markdown","source":"#### Q 1.2.3\nConsider an alternative implementation to detect if L is a palindrome and write a new function, `is_palindrome2(L)`."},{"metadata":{},"cell_type":"markdown","source":"#### Q 1.2.4\nWhat's the complexity of this implementation?\n"},{"metadata":{},"cell_type":"markdown","source":"#### Q 1.2.5 \nWhat are some examples of optimizations that could improve computational performance?\n"},{"metadata":{},"cell_type":"markdown","source":"## 2. Prob + Stats"},{"metadata":{},"cell_type":"markdown","source":"### Q 2.1: Finding $\\pi$ in a random uniform?\n<img src=https://www.epicurus.com/food/recipes/wp-content/uploads/2015/03/Pi-Day.jpg width=\"480\">\n\nGiven a uniform random generator $[0,1)$ (e.g., use your language's standard libary to generate random value), write a a function `compute_pi` to compute [$\\pi$](https://en.wikipedia.org/wiki/Pi)."},{"metadata":{},"cell_type":"markdown","source":"### Q 2.2: Making a 6-side die roll a 7?\n\nUsing a single 6-side die, how can you generate a random number between 1 - 7?"},{"metadata":{},"cell_type":"markdown","source":"### Q 2.3: Is normality uniform?\n\n<img src=https://rednaxela1618.files.wordpress.com/2014/06/uniformnormal.png width=\"480\">\n\n\nGiven draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?"},{"metadata":{},"cell_type":"markdown","source":"### Q 2.4: Should you pay or should you go?\n\n![coin flip](https://lh5.ggpht.com/iwD6MnHeHVAXNBgrO7r4N9MQxxYi6wT9vb0Mqu905zTnNlBciONAA98BqafyjzC06Q=w300)\n\nLet’s say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you $2^{(n-1)}$ US dollars. How much would you pay me to play this game? Explain."},{"metadata":{},"cell_type":"markdown","source":"### Q 2.5: Uber vs. Lyft\n\n![uber vs lyft](http://usiaffinity.typepad.com/.a/6a01347fc1cb08970c01bb0876bcbe970d-pi)\n\nYou request 2 UberX’s and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberX’s arrive first?"},{"metadata":{},"cell_type":"markdown","source":"### Q 2.6: Pick your prize\n<img src=https://miro.medium.com/max/1100/1*m5b3O9sE68UCXjLw5oxy2g.png width=\"480\">\n\nA prize is placed at random behind one of three doors and you are asked to pick a door. To be concrete, say you always pick door 1. Now the game host chooses one of door 2 or 3, opens it and shows you that it is empty. They then give you the option to keep your picked door or switch to the unopened door. Should you stay or switch if you want to maximize your probability of winning the prize?"},{"metadata":{},"cell_type":"markdown","source":"## 3 Conceptual ML"},{"metadata":{},"cell_type":"markdown","source":"### Q 3.1 Why study gradient boosting or neural networks?\n\nConsider a regression setting where $X \\in \\mathbb{R}^p$ and $Y \\in \\mathbb{R}$. The goal is to come up with a function $f(X): \\mathbb{R}^p \\rightarrow \\mathbb{R}$ that minimizes the squared-error loss $(Y - f(X))^2$. Since X, Y are random variables, we seek to minimize the expectation of the squared error loss as follows\n\\begin{equation}\nEPE(f) = \\mathbb{E}\\left[(Y-f(X)^2\\right]\n\\end{equation}\nwhere EPE stands for expected prediction error. One can show that minimizing the expected prediction error leads to the following _regression function_\n\\begin{equation}\nf(x) = \\mathbb{E}\\left[Y|X=x\\right]\n\\end{equation}\n\nThe goal of any method is to approximate the regression function above, which we denote as $\\hat{f}(x)$. For example, linear regression explicitly assumes that the regression function is approximately linear in its arguments, i.e. $\\hat{f}(x) = x^T\\beta$ while a neural network provides a nonlinear approximation of the regression function. \n\nThe simplest of all these methods is [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). Given $x$ and some neighbourhood of $k$ points $N_k(x)$, $\\hat{f}(x)$ is simply the average of all $y_i|x_i \\in N_k(x)$.  Let $N$ denote the training sample size. Under mild regularity conditions on the joint probability distribution $Pr(X, Y)$, one can show that as $N \\rightarrow \\infty$, $k \\rightarrow \\infty$ such that $k/N \\rightarrow 0$, then $\\hat{f}(x) \\rightarrow f(x)$ where $\\rightarrow$ means approaches or goes to. In other words, the k-nearest neighbors algorithm converges to the ideal solution as both the training sample size and number of neighbors increase to infinity.\n\nNow given this _universal approximator_, why look any further and research other methods? Please share your thoughts.\n"},{"metadata":{},"cell_type":"markdown","source":"### Q 3.2 Model Selection and Assesment\n\nConsider a multiclass classification problem with a large number of features $p >> N$, for e.g $p=10000, N=100$ The task is threefold\n1. Find a \"good\" subset of features that show strong _univariate_ correlation with class labels\n2. Using the \"good\" subset, build a multi class classifier\n3. Estimate the generalization error of the final model\n\nGiven this dataset, outline your approach and please be sure to cover the following\n- Data splitting\n- Model Selection: either estimating the performance of different classifiers or the same classifier with different hyperparameters\n- Model Assessment: having chosen a classifier, estimating the generalization error\n\nAssume all features are numerical, the dataset contains no NULLS, outliers, etc. and doesn't require any preprocessing.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Q 1.1: Fibionacci\n\ndef fibonacci(n):\n  if n < 2:\n    return n \n  return fibonacci(n-1) + fibonacci(n-2)\n\n# Complexity of solution is O(2^n)\n\n# alternate approach by storing past values \ndef fibonacci2(n):\n  fibo = [0, 1]\n  for i in range(2,n+1):\n    fibo.append(fibo[i-1] + fibo[i-2])\n  return fibo[n]\n\n# Complexity of solution is O(n)\n\n# In the first recursive approach method calls itself repeatedly for lower values even though they have been previosuly \n# calculated thus for high values it will take a lot of time to compute. Whereas in the second approach the method stores\n# previously computed values therefore they are computed once and it has a time complexity of O(n).\n# Further optimization can be done using other approaches to reduce complexity to O(logn) or using direct formula","execution_count":10,"outputs":[{"output_type":"stream","text":"8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Question 2.5\n\n# Probablity of Lyfts arriving first:\n\n# In total there are 5 cars on the way out of which 3 are lyft and 2 are uber\n# for first lyft probability is 3/5 \n# for second lyft it is 2/4\n# for third lyft it is 1/3\n\n# so probability that all lyfts arrive first is (3/5)*(2/4)*(1/3) = 1/10\n\n# Similarly for ubers\n# for first uber probability is 2/5 \n# for second lyft it is 1/4\n\n# so probability that all uber arrive first is (2/5)*(1/4) = 1/10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Question 3.6\n\n# Find a \"good\" subset of features that show strong univariate correlation with class labels\n# Using the \"good\" subset, build a multi class classifier\n# Estimate the generalization error of the final model\n\n# i)\n# In the current scenario we have p>>>n; or eg 10000 features and 100 samples\n# Since, the dimension of features is very high comapred to samples it doesn't make sense to use so many features for a multi-class classification problem.\n# Therfore, we have to reduce the number of dimensions. There several filtering approaches to reduce the features.\n\n# One approach is to check the correlation between all the features. A feature having high correlation with classes and low correlation with other features is a good feature. \n# Thus, we can utilize technique like pearson correlation which gives correlation in rangw -1 to 1 (1 being high positive correlation and -1 being high negative correlation).\n# Based on the correlation value best features can be filtered from the data.\n\n# Another approach is to calculate Anova F-value for each feature and target. Anova is analysis of variance, it determines the difference between target and feature. Further, features \n# with higher F-value can be selected for model training.\n\n# Both the approaches can be tried to find out good subset and which works better.\n\n\n# ii)\n# Selecting a classifier can be done on the basis of number of samples and features. Here, the number of features is very very high compared to samples therefore simple models \n# like Naive Bayes, Logistic Regression and Linear SVC would yield better results. Models like Multi-Layer Perceptron, Decision Trees and KNN would be more effective with more samples.\n# Naive Bayes is good baseline model it performs well even with less but it's not the best model. Logistic regression is ideal for binary classification as it generates a linear boundary to\n# seperate classes but here it is a multi-class classification problem therefore it isn't ideal. Finally, Linear SVC is potentially the best model. It will generate a hyperplane to segregate \n# the data into multiple classes. \n\n# iii)\n# Since the number of samples is very less, generalization is difficult in this scenario. Generalization error will be higher as the model will memorize the values and give a biased result. \n# For better generalization we can use cross-validation to create a sense of unseen data by splitting data randomly into folds.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}